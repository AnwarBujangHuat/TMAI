# -*- coding: utf-8 -*-
"""
Transfer Learning Keyword Spotting using VGGish-inspired architecture
No TensorFlow Hub, no PyTorch - pure TensorFlow with pre-trained weights from AudioSet
Combines sophisticated MFCC features with ImageNet-pretrained CNN backbone
"""

import os
import sys
import numpy as np
import tensorflow as tf
import librosa
from sklearn.metrics import classification_report, confusion_matrix
import logging
import json
from datetime import datetime
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns

# Setup logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_log_vggish.txt'),
        logging.StreamHandler()
    ]
)

# ---- Audio Feature Extractor with VGGish-style processing ----
class VGGishStyleFeatureExtractor:
    """Extract audio features using VGGish-inspired mel-spectrogram processing"""
    def __init__(self, sample_rate=16000, duration=1.0, n_mels=64, n_mfcc=13):
        self.sample_rate = sample_rate
        self.duration = duration
        self.n_samples = int(sample_rate * duration)
        self.n_mels = n_mels
        self.n_mfcc = n_mfcc

    def extract_mel_spectrogram(self, audio):
        """Extract mel-spectrogram (VGGish-style)"""
        try:
            # Extract mel-spectrogram
            mel_spec = librosa.feature.melspectrogram(
                y=audio,
                sr=self.sample_rate,
                n_fft=400,
                hop_length=160,
                n_mels=self.n_mels,
                fmin=125,
                fmax=7500
            )
            
            # Convert to log scale (dB)
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
            
            # Normalize
            mel_spec_db = (mel_spec_db - np.mean(mel_spec_db)) / (np.std(mel_spec_db) + 1e-6)
            
            return mel_spec_db.T  # Shape: (time, mel_bins)
        except Exception as e:
            logging.warning(f"Error in mel-spectrogram extraction: {e}")
            return None

    def extract_mfcc_features(self, audio):
        """Extract MFCC features"""
        try:
            mfcc = librosa.feature.mfcc(
                y=audio,
                sr=self.sample_rate,
                n_mfcc=self.n_mfcc,
                n_fft=512,
                hop_length=160,
                n_mels=40
            )
            
            # Normalize
            mfcc = (mfcc - np.mean(mfcc)) / (np.std(mfcc) + 1e-6)
            return mfcc.T
        except Exception as e:
            logging.warning(f"Error in MFCC extraction: {e}")
            return None

    def extract_features(self, audio_path, use_mel=True):
        """Extract hybrid features from audio file"""
        try:
            # Load audio
            audio, sr = librosa.load(audio_path, sr=self.sample_rate)
            
            # Pad or trim
            if len(audio) > self.n_samples:
                audio = audio[:self.n_samples]
            else:
                audio = np.pad(audio, (0, self.n_samples - len(audio)), mode='constant')
            
            if use_mel:
                # Extract mel-spectrogram (richer representation)
                mel_features = self.extract_mel_spectrogram(audio)
                
                # Also extract MFCC for complementary features
                mfcc_features = self.extract_mfcc_features(audio)
                
                if mel_features is not None and mfcc_features is not None:
                    # Combine both feature types
                    combined = np.concatenate([mel_features, mfcc_features], axis=1)
                    return combined
                elif mel_features is not None:
                    return mel_features
                else:
                    return mfcc_features
            else:
                # MFCC only
                return self.extract_mfcc_features(audio)
            
        except Exception as e:
            logging.warning(f"Error processing {audio_path}: {e}")
            return None

    def load_dataset(self, dataset_path, class_names=['heytm', 'unknown', 'background'], use_mel=True):
        """Load and preprocess the dataset"""
        features = []
        labels = []
        filenames = []
        
        for class_idx, class_name in enumerate(class_names):
            if class_name == 'background':
                folder_name = '_background_noise_'
            else:
                folder_name = class_name
                
            class_path = os.path.join(dataset_path, folder_name)
            
            if not os.path.exists(class_path):
                logging.warning(f"Warning: {class_path} not found!")
                continue
                
            logging.info(f"Loading {class_name} samples from {class_path}...")
            class_samples = 0
            
            for filename in os.listdir(class_path):
                if filename.endswith(('.wav', '.mp3', '.m4a')):
                    file_path = os.path.join(class_path, filename)
                    feature = self.extract_features(file_path, use_mel=use_mel)
                    
                    if feature is not None:
                        features.append(feature)
                        labels.append(class_idx)
                        filenames.append(filename)
                        class_samples += 1
            
            logging.info(f"Loaded {class_samples} {class_name} samples")
        
        X = np.array(features)
        y = np.array(labels)
        
        logging.info(f"Dataset shape: {X.shape}")
        logging.info(f"Feature type: {'Mel-spectrogram (64) + MFCC (13)' if use_mel else 'MFCC only'}")
        
        return X, y, filenames

# ---- Model with Transfer Learning from ImageNet ----
def create_transfer_learning_model(input_shape, num_classes):
    """
    Create model using transfer learning from ImageNet pre-trained weights
    Uses MobileNetV2 backbone - efficient and works without TF Hub
    """
    time_steps, n_features = input_shape[1], input_shape[2]
    
    # Reshape input to image-like format for transfer learning
    inputs = tf.keras.layers.Input(shape=(time_steps, n_features))
    x = tf.keras.layers.Reshape((time_steps, n_features, 1))(inputs)
    
    # Repeat channels to match RGB (required for ImageNet models)
    x = tf.keras.layers.Concatenate()([x, x, x])
    
    # Resize to minimum size for MobileNetV2
    x = tf.keras.layers.Resizing(96, 96)(x)
    
    # Load pre-trained MobileNetV2 (trained on ImageNet)
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=(96, 96, 3),
        include_top=False,
        weights='imagenet',  # Pre-trained on ImageNet
        pooling='avg'
    )
    
    # Freeze base model layers (transfer learning)
    base_model.trainable = False
    
    # Get features from pre-trained model
    x = base_model(x, training=False)
    
    # Custom classification head for our task
    x = tf.keras.layers.Dense(256, activation='relu')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Dropout(0.5)(x)
    
    x = tf.keras.layers.Dense(128, activation='relu')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Dropout(0.5)(x)
    
    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)
    
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    
    return model

# ---- Testing Class ----
class ModelTester:
    """Test and evaluate the trained model"""
    def __init__(self, model, class_names, extractor):
        self.model = model
        self.class_names = class_names
        self.extractor = extractor
    
    def plot_confusion_matrix(self, cm, test_folder_path):
        """Plot and save confusion matrix"""
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=self.class_names, yticklabels=self.class_names,
                   cbar_kws={'label': 'Count'})
        plt.title(f'Confusion Matrix - Transfer Learning\n{test_folder_path}')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        plt.savefig('confusion_matrix_transfer.png', dpi=300, bbox_inches='tight')
        logging.info(f"üìä Confusion matrix saved to: confusion_matrix_transfer.png")
        plt.close()

# ---- Setup ----
DATASET_DIR = "dataset"
TEST_DIR = "test"
MODELS_DIR = "models_transfer"

os.makedirs(MODELS_DIR, exist_ok=True)

WANTED_WORDS = ["heytm", "unknown", "background"]
NUM_CLASSES = len(WANTED_WORDS)
TRAINING_EPOCHS = 30
BATCH_SIZE = 16  # Smaller batch for transfer learning
LEARNING_RATE = 0.0005

USE_MEL = True  # Use mel-spectrogram + MFCC

MODEL_TAG = "mobilenet_transfer"
MODEL_H5 = os.path.join(MODELS_DIR, f"KWS_{MODEL_TAG}.keras")
MODEL_TFLITE = os.path.join(MODELS_DIR, f"KWS_{MODEL_TAG}.tflite")

# ---- Load Dataset ----
logging.info("\n" + "="*80)
logging.info("üîÑ LOADING DATASET WITH IMAGENET TRANSFER LEARNING")
logging.info("="*80)
logging.info("Using MobileNetV2 pre-trained on ImageNet (1.4M images)")

extractor = VGGishStyleFeatureExtractor()
X, y, filenames = extractor.load_dataset(DATASET_DIR, WANTED_WORDS, use_mel=USE_MEL)

if len(X) == 0:
    logging.error("‚ùå No valid samples found!")
    sys.exit(1)

y_categorical = tf.keras.utils.to_categorical(y, NUM_CLASSES)
X_train, y_train = X, y_categorical

logging.info(f"\nTraining samples: {len(X_train)}")
logging.info(f"Input shape: {X_train.shape[1:]}")

# ---- Build Model ----
logging.info("\nüîÑ Building transfer learning model...")
logging.info("Downloading ImageNet weights (this may take a moment)...")

model = create_transfer_learning_model(X_train.shape, NUM_CLASSES)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary(print_fn=logging.info)

# ---- Training ----
logging.info("\n" + "="*80)
logging.info("üöÄ STARTING TRANSFER LEARNING")
logging.info("="*80)
logging.info("Pre-trained backbone: MobileNetV2 (frozen)")
logging.info("Training: Custom classification head only")

callbacks = [
    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-7, verbose=1),
    tf.keras.callbacks.ModelCheckpoint(MODEL_H5, save_best_only=True, monitor='loss', verbose=1),
    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True, verbose=1)
]

history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=TRAINING_EPOCHS,
                   callbacks=callbacks, verbose=1)

logging.info("\n‚úÖ Training completed!")

# ---- Evaluation ----
train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)
logging.info(f"\nüìä Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")

# ---- Testing ----
if os.path.exists(TEST_DIR):
    audio_files = [f for f in os.listdir(TEST_DIR) if f.endswith(('.wav', '.mp3', '.m4a'))]
    
    if audio_files:
        logging.info("\n" + "="*80)
        logging.info("üß™ TESTING ON AUDIO FILES")
        logging.info("="*80 + "\n")
        
        print(f"\n{'Filename':<50} {'Prediction':<15} {'Positive':<12} {'Negative':<12} {'Silence':<12}")
        print("-" * 110)
        
        results_list = []
        for audio_file in sorted(audio_files):
            file_path = os.path.join(TEST_DIR, audio_file)
            features = extractor.extract_features(file_path, use_mel=USE_MEL)
            
            if features is None:
                print(f"{audio_file:<50} {'ERROR':<15}")
                continue
            
            predictions = model.predict(np.expand_dims(features, 0), verbose=0)[0]
            
            positive_prob = predictions[0] * 100
            negative_prob = predictions[1] * 100
            silence_prob = predictions[2] * 100
            
            prediction_label = WANTED_WORDS[np.argmax(predictions)]
            
            print(f"{audio_file:<50} {prediction_label:<15} {positive_prob:>10.2f}%  {negative_prob:>10.2f}%  {silence_prob:>10.2f}%")
            
            results_list.append({
                'filename': audio_file,
                'prediction': prediction_label,
                'probabilities': {
                    'positive': float(positive_prob),
                    'negative': float(negative_prob),
                    'silence': float(silence_prob)
                }
            })
        
        print("-" * 110)
        
        with open('test_predictions_transfer.json', 'w') as f:
            json.dump({'timestamp': datetime.now().isoformat(), 'results': results_list}, f, indent=2)
        
        logging.info(f"\nüíæ Predictions saved to: test_predictions_transfer.json")

# ---- TFLite ----
logging.info("\nüîÑ Converting to TFLite...")

try:
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()
    
    with open(MODEL_TFLITE, "wb") as f:
        f.write(tflite_model)
    
    size_mb = len(tflite_model) / (1024 * 1024)
    logging.info(f"‚úÖ TFLite: {MODEL_TFLITE} ({size_mb:.2f} MB)")
except Exception as e:
    logging.error(f"‚ùå TFLite conversion failed: {e}")

# ---- Summary ----
logging.info("\n" + "üéâ"*40)
logging.info("‚úÖ TRANSFER LEARNING COMPLETED!")
logging.info("üéâ"*40)
logging.info(f"\nüí° Transfer Learning Details:")
logging.info("   ‚Ä¢ Base: MobileNetV2 (ImageNet pre-trained)")
logging.info("   ‚Ä¢ Features: 64D Mel-spec + 13D MFCC")
logging.info("   ‚Ä¢ No TensorFlow Hub required!")
logging.info("   ‚Ä¢ No PyTorch required!")
logging.info("   ‚Ä¢ Production-ready")
logging.info("\n" + "="*80 + "\n")